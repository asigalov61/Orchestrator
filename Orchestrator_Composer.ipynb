{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Orchestrator Composer (ver. 2.0)\n",
        "\n",
        "***\n",
        "\n",
        "Powered by tegridy-tools: https://github.com/asigalov61/tegridy-tools\n",
        "\n",
        "***\n",
        "\n",
        "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please excercise great humility, care, and respect. https://www.nscai.gov/\n",
        "\n",
        "***\n",
        "\n",
        "#### Project Los Angeles\n",
        "\n",
        "#### Tegridy Code 2022\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "gpy3qsulqHa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (GPU CHECK)"
      ],
      "metadata": {
        "id": "W_So4w8fqPGL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3rABEpKCO02",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title NVIDIA GPU check\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (SETUP ENVIRONMENT)"
      ],
      "metadata": {
        "id": "C0XxnXGFqVyh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK40g6V_BTNj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies\n",
        "!git clone --depth 1 https://github.com/asigalov61/Orchestrator\n",
        "!pip install torch\n",
        "!pip install einops\n",
        "!pip install torch-summary\n",
        "!pip install sklearn\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "!apt install fluidsynth #Pip does not work for some reason. Only apt works\n",
        "!pip install midi2audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzCOZU_gBiQV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import modules\n",
        "\n",
        "print('=' * 70)\n",
        "print('Loading core Orchestrator modules...')\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import secrets\n",
        "import statistics\n",
        "from time import time\n",
        "import tqdm\n",
        "\n",
        "print('=' * 70)\n",
        "print('Loading main Orchestrator modules...')\n",
        "import torch\n",
        "\n",
        "%cd /content/Orchestrator\n",
        "\n",
        "import TMIDIX\n",
        "\n",
        "from lwa_transformer import *\n",
        "\n",
        "%cd /content/\n",
        "print('=' * 70)\n",
        "print('Loading aux Orchestrator modeules...')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchsummary import summary\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "from midi2audio import FluidSynth\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print('=' * 70)\n",
        "print('Done!')\n",
        "print('Enjoy! :)')\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI3aQtHzqSnp"
      },
      "source": [
        "# (LOAD MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip Pre-Trained Orchestrator Model\n",
        "print('=' * 70)\n",
        "%cd /content/Orchestrator/Model\n",
        "\n",
        "print('=' * 70)\n",
        "print('Unzipping pre-trained Orchestartor model...Please wait...')\n",
        "\n",
        "!cat /content/Orchestrator/Model/Orchestrator_Trained_Model_55253_steps_0.3277_loss.zip* > /content/Orchestrator/Model/Orchestrator_Trained_Model_55253_steps_0.3277_loss.zip\n",
        "print('=' * 70)\n",
        "\n",
        "!unzip -j /content/Orchestrator/Model/Orchestrator_Trained_Model_55253_steps_0.3277_loss.zip\n",
        "print('=' * 70)\n",
        "\n",
        "print('Done! Enjoy! :)')\n",
        "print('=' * 70)\n",
        "%cd /content/\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SqbxcFYVq63r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDquonbXC2je",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load Orchestrator Pre-Trained Model\n",
        "full_path_to_model_checkpoint = \"/content/Orchestrator/Model/Orchestrator_Trained_Model_55253_steps_0.3277_loss.pth\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Model precision option\n",
        "\n",
        "model_precision = \"float16\" # @param [\"float16\", \"float32\"]\n",
        "\n",
        "#@markdown float16 == Half precision/double speed\n",
        "\n",
        "#@markdown float32 == Full precision/normal speed\n",
        "\n",
        "plot_tokens_embeddings = False # @param {type:\"boolean\"}\n",
        "\n",
        "print('=' * 70)\n",
        "print('Loading Orchestrator Pre-Trained Model...')\n",
        "print('Please wait...')\n",
        "print('=' * 70)\n",
        "print('Instantiating model...')\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[model_precision]\n",
        "ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "SEQ_LEN = 4096\n",
        "\n",
        "# instantiate the model\n",
        "\n",
        "model = LocalTransformer(\n",
        "    num_tokens = 774,\n",
        "    dim = 1024,\n",
        "    depth = 24,\n",
        "    causal = True,\n",
        "    local_attn_window_size = 512,\n",
        "    max_seq_len = SEQ_LEN\n",
        ").cuda()\n",
        "print('=' * 70)\n",
        "\n",
        "print('Loading model checkpoint...')\n",
        "\n",
        "model.load_state_dict(torch.load(full_path_to_model_checkpoint))\n",
        "print('=' * 70)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print('Done!')\n",
        "print('=' * 70)\n",
        "\n",
        "# Model stats\n",
        "print('Model summary...')\n",
        "summary(model)\n",
        "\n",
        "# Plot Token Embeddings\n",
        "\n",
        "if plot_tokens_embeddings:\n",
        "  tok_emb = model.token_emb.weight.detach().cpu().tolist()\n",
        "\n",
        "  cos_sim = metrics.pairwise_distances(\n",
        "    tok_emb, metric='cosine'\n",
        "  )\n",
        "  plt.figure(figsize=(7, 7))\n",
        "  plt.imshow(cos_sim, cmap=\"inferno\", interpolation=\"nearest\")\n",
        "  im_ratio = cos_sim.shape[0] / cos_sim.shape[1]\n",
        "  plt.colorbar(fraction=0.046 * im_ratio, pad=0.04)\n",
        "  plt.xlabel(\"Position\")\n",
        "  plt.ylabel(\"Position\")\n",
        "  plt.tight_layout()\n",
        "  plt.plot()\n",
        "  plt.savefig(\"/content/Orchestrator-Tokens-Embeddings-Plot.png\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (LOAD SEED MIDI)"
      ],
      "metadata": {
        "id": "Gt03VtO6uKkb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QXbFLsKqSnt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load Seed MIDI\n",
        "select_seed_MIDI = \"Upload your own custom MIDI\" #@param [\"Upload your own custom MIDI\", \"Orchestrator-Piano-Seed-1\", \"Orchestrator-Piano-Seed-2\", \"Orchestrator-Piano-Seed-3\", \"Orchestrator-Piano-Seed-4\", \"Orchestrator-Piano-Seed-5\", \"Orchestrator-MI-Seed-1\", \"Orchestrator-MI-Seed-2\", \"Orchestrator-MI-Seed-3\", \"Orchestrator-MI-Seed-4\", \"Orchestrator-MI-Seed-5\"]\n",
        "number_of_prime_tokens = 384 # @param {type:\"slider\", min:384, max:8192, step:4}\n",
        "render_MIDI_to_audio = False # @param {type:\"boolean\"}\n",
        "\n",
        "if select_seed_MIDI != \"Upload your own custom MIDI\":\n",
        "  f = '/content/Orchestrator/Seeds/'+select_seed_MIDI+'.mid'\n",
        "  score = TMIDIX.midi2ms_score(open(f, 'rb').read())\n",
        "\n",
        "else:\n",
        "  uploaded_MIDI = files.upload()\n",
        "  score = TMIDIX.midi2ms_score(list(uploaded_MIDI.values())[0])\n",
        "  f = list(uploaded_MIDI.keys())[0]\n",
        "\n",
        "\n",
        "print('=' * 70)\n",
        "print('Orchestrator Seed MIDI Loader')\n",
        "print('=' * 70)\n",
        "print('Loading seed MIDI...')\n",
        "print('=' * 70)\n",
        "print('File:', f)\n",
        "print('=' * 70)\n",
        "#=======================================================\n",
        "\n",
        "transpose_to_model_average_pitch = False\n",
        "\n",
        "#=======================================================\n",
        "# START PROCESSING\n",
        "\n",
        "# INSTRUMENTS CONVERSION CYCLE\n",
        "events_matrix = []\n",
        "melody_chords_f = []\n",
        "melody_chords_f1 = []\n",
        "itrack = 1\n",
        "patches = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "patch_map = [[0, 1, 2, 3, 4, 5, 6, 7], # Piano\n",
        "          [24, 25, 26, 27, 28, 29, 30], # Guitar\n",
        "          [32, 33, 34, 35, 36, 37, 38, 39], # Bass\n",
        "          [40, 41], # Violin\n",
        "          [42, 43], # Cello\n",
        "          [46], # Harp\n",
        "          [56, 57, 58, 59, 60], # Trumpet\n",
        "          [71, 72], # Clarinet\n",
        "          [73, 74, 75], # Flute\n",
        "          [-1], # Drums\n",
        "          [52, 53], # Choir\n",
        "          [16, 17, 18, 19, 20] # Organ\n",
        "          ]\n",
        "\n",
        "while itrack < len(score):\n",
        "  for event in score[itrack]:\n",
        "      if event[0] == 'note' or event[0] == 'patch_change':\n",
        "          events_matrix.append(event)\n",
        "  itrack += 1\n",
        "\n",
        "events_matrix.sort(key=lambda x: x[1])\n",
        "\n",
        "events_matrix1 = []\n",
        "\n",
        "for event in events_matrix:\n",
        "  if event[0] == 'patch_change':\n",
        "      patches[event[2]] = event[3]\n",
        "\n",
        "  if event[0] == 'note':\n",
        "      event.extend([patches[event[3]]])\n",
        "      once = False\n",
        "\n",
        "      for p in patch_map:\n",
        "          if event[6] in p and event[3] != 9: # Except the drums\n",
        "              event[3] = patch_map.index(p)\n",
        "              once = True\n",
        "\n",
        "      if not once and event[3] != 9: # Except the drums\n",
        "          event[3] = 15 # All other instruments/patches channel\n",
        "          event[5] = max(80, event[5])\n",
        "\n",
        "      if event[3] < 12: # We won't write chans 12-16 for now...\n",
        "          events_matrix1.append(event)\n",
        "          # stats[event[3]] += 1\n",
        "\n",
        "#=======================================================\n",
        "# PRE-PROCESSING\n",
        "\n",
        "# checking number of instruments in a composition\n",
        "instruments_list_without_drums = list(set([y[3] for y in events_matrix1 if y[3] != 9]))\n",
        "\n",
        "instruments_list = list(set([y[3] for y in events_matrix1]))\n",
        "num_instr = len(instruments_list)\n",
        "\n",
        "# filtering out empty compositions and checking desired number of instruments in a composition\n",
        "# It had been observed that music models learn best from multi-instrumental music, even for solo instruments\n",
        "# So you can setup filtering by number of instruments here if you want\n",
        "\n",
        "if len(events_matrix1) > 0 and len(instruments_list_without_drums) > 0:\n",
        "\n",
        "    # recalculating timings\n",
        "    for e in events_matrix1:\n",
        "        e[1] = int(e[1] / 8) # Max 1 seconds for start-times\n",
        "        e[2] = int(e[2] / 16) # Max 2 seconds for durations\n",
        "\n",
        "    # Sorting by pitch, then by start-time\n",
        "    events_matrix1.sort(key=lambda x: x[4], reverse=True)\n",
        "    events_matrix1.sort(key=lambda x: x[1])\n",
        "\n",
        "    # pitches augment stuff (calculating transpose value to C4 without drums)\n",
        "    pitches = [y[4] for y in events_matrix1 if y[3] != 9]\n",
        "\n",
        "    if len(pitches) > 0:\n",
        "      avg_ptc = round(statistics.mean(pitches))\n",
        "    else:\n",
        "      avg_ptc = 0\n",
        "\n",
        "    ptc_delta = 12 - (avg_ptc % 12)\n",
        "\n",
        "    #=======================================================\n",
        "    # FINAL PRE-PROCESSING\n",
        "\n",
        "    first_event = True\n",
        "    melody_chords = []\n",
        "    pe = events_matrix1[0]\n",
        "    pt = -1\n",
        "\n",
        "    for e in events_matrix1:\n",
        "\n",
        "        # Cliping all values...\n",
        "        tim = max(0, min(127, e[1]-pe[1]))\n",
        "        dur = max(1, min(127, e[2]))\n",
        "        cha = max(0, min(11, e[3]))\n",
        "        ptc = max(1, min(127, e[4]))\n",
        "        vel = max(8, min(127, e[5]))\n",
        "\n",
        "        velocity = round(vel / 15)\n",
        "\n",
        "        # Pitches shifting\n",
        "        if cha != 9:\n",
        "            if transpose_to_model_average_pitch:\n",
        "                ptc_aug = ptc + ptc_delta # Transposing composition to median C4\n",
        "            else:\n",
        "                ptc_aug = ptc\n",
        "        else:\n",
        "          ptc_aug = ptc + 128 # Shifting drums pitches because drums structure is different from non-drums\n",
        "\n",
        "        # Time shifting\n",
        "        if tim == 0 and first_event == True:\n",
        "          abs_time = 0\n",
        "\n",
        "        if tim == 0 and first_event == False:\n",
        "          if pt == 0:\n",
        "            pass\n",
        "          else:\n",
        "            abs_time += 128\n",
        "\n",
        "        if tim !=0:\n",
        "          abs_time = tim\n",
        "\n",
        "        # Writing final note\n",
        "        melody_chords.append([abs_time, dur, cha, ptc_aug, velocity])\n",
        "\n",
        "        pe = e\n",
        "        pt = tim\n",
        "\n",
        "        first_event = False\n",
        "\n",
        "    #=======================================================\n",
        "    # FINAL PROCESSING\n",
        "    #=======================================================\n",
        "\n",
        "    # Break between compositions / Intro seq\n",
        "\n",
        "    # 758 == SOS/EOS token\n",
        "    # 759 == SOS/EOS token\n",
        "    # 760-761 == Composition is without drums or with drums\n",
        "    # 762-773 == Number of instruments in a composition\n",
        "\n",
        "    # TOTAL DICTIONARY SIZE OF 774 TOKENS\n",
        "\n",
        "    if 9 in instruments_list:\n",
        "      drums_present = 761 # Yes\n",
        "    else:\n",
        "      drums_present = 760 # No\n",
        "\n",
        "    melody_chords_f.extend([758, 759, drums_present, 762+(num_instr-1)])\n",
        "    melody_chords_f1.append([758, 759, drums_present, 762+(num_instr-1)])\n",
        "\n",
        "    #=======================================================\n",
        "\n",
        "    # Composition control seq\n",
        "    intro_mode_time = statistics.mode([y[0] for y in melody_chords if y[2] != 9])\n",
        "    intro_mode_dur = statistics.mode([y[1] for y in melody_chords if y[2] != 9])\n",
        "    intro_mode_pitch = statistics.mode([y[3] for y in melody_chords if y[2] != 9])\n",
        "    intro_mode_velocity = statistics.mode([y[4] for y in melody_chords if y[2] != 9])\n",
        "\n",
        "    # Instrument value 12 is reserved for composition control seq\n",
        "    intro_vel = (12 * 9) + intro_mode_velocity\n",
        "\n",
        "    melody_chords_f.extend([intro_mode_time, intro_mode_dur+256, intro_mode_pitch+384, intro_vel+640])\n",
        "    melody_chords_f1.append([intro_mode_time, intro_mode_dur+256, intro_mode_pitch+384, intro_vel+640])\n",
        "\n",
        "    #=======================================================\n",
        "    # MAIN PROCESSING CYCLE\n",
        "    #=======================================================\n",
        "\n",
        "    for m in melody_chords:\n",
        "\n",
        "        # WRITING EACH NOTE HERE\n",
        "        chan_vel = (m[2] * 9) + m[4]\n",
        "        melody_chords_f.extend([m[0], m[1]+256, m[3]+384, chan_vel+640])\n",
        "        melody_chords_f1.append([m[0], m[1]+256, m[3]+384, chan_vel+640])\n",
        "\n",
        "    melody_chords_f1 = melody_chords_f1[:(number_of_prime_tokens // 4)]\n",
        "    melody_chords_f = melody_chords_f[:number_of_prime_tokens]\n",
        "\n",
        "    #=======================================================\n",
        "\n",
        "    song = melody_chords_f\n",
        "    song_f = []\n",
        "    tim = 0\n",
        "    dur = 0\n",
        "    vel = 0\n",
        "    pitch = 0\n",
        "    channel = 0\n",
        "\n",
        "    son = []\n",
        "    song1 = []\n",
        "\n",
        "    for s in song:\n",
        "      if s > 256 and s < (12*9)+640:\n",
        "        son.append(s)\n",
        "      else:\n",
        "        if len(son) == 4:\n",
        "          song1.append(son)\n",
        "        son = []\n",
        "        son.append(s)\n",
        "\n",
        "\n",
        "    for ss in song1:\n",
        "\n",
        "        if ss[0] < 128:\n",
        "          tim += ss[0] * 8\n",
        "\n",
        "        dur = (ss[1]-256) * 16\n",
        "\n",
        "        if (ss[2]-384) > 128:\n",
        "          pitch = (ss[2]-384) - 128\n",
        "        else:\n",
        "          pitch = (ss[2]-384)\n",
        "\n",
        "\n",
        "        channel = (ss[3]-640) // 9\n",
        "        vel = ((ss[3]-640) % 9) * 15\n",
        "\n",
        "        song_f.append(['note', tim, dur, channel, pitch, vel ])\n",
        "\n",
        "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
        "                                                          output_signature = 'Orchestrator',\n",
        "                                                          output_file_name = '/content/Orchestrator-Seed-Composition',\n",
        "                                                          track_name='Project Los Angeles',\n",
        "                                                          list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],\n",
        "                                                          number_of_ticks_per_quarter=500)\n",
        "\n",
        "    #=======================================================\n",
        "\n",
        "print('=' * 70)\n",
        "print('Composition stats:')\n",
        "print('Composition has', len(melody_chords_f1), 'notes')\n",
        "print('Composition has', len(melody_chords_f), 'tokens')\n",
        "print('=' * 70)\n",
        "\n",
        "print('Displaying resulting composition...')\n",
        "print('=' * 70)\n",
        "\n",
        "fname = '/content/Orchestrator-Seed-Composition'\n",
        "\n",
        "x = []\n",
        "y =[]\n",
        "c = []\n",
        "\n",
        "colors = ['red', 'yellow', 'green', 'cyan', 'blue', 'pink', 'orange', 'purple', 'gray', 'white', 'gold', 'silver']\n",
        "\n",
        "block_lines = [(song_f[-1][1] / 1000)]\n",
        "\n",
        "for s in song_f:\n",
        "  x.append(s[1] / 1000)\n",
        "  y.append(s[4])\n",
        "  c.append(colors[s[3]])\n",
        "\n",
        "if render_MIDI_to_audio:\n",
        "  FluidSynth(\"/usr/share/sounds/sf2/FluidR3_GM.sf2\", 16000).midi_to_audio(str(fname + '.mid'), str(fname + '.wav'))\n",
        "  display(Audio(str(fname + '.wav'), rate=16000))\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "ax=plt.axes(title=fname)\n",
        "ax.set_facecolor('black')\n",
        "\n",
        "plt.scatter(x,y, c=c)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Pitch\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (COMPOSITION LOOP)\n",
        "\n",
        "## Run the cells below in a loop to generate endless continuation"
      ],
      "metadata": {
        "id": "7xNyANjZsCOi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkvXYwR_qSnx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Standard/Simple Continuation\n",
        "\n",
        "#@markdown Generation settings\n",
        "\n",
        "number_of_batches_to_generate = 4 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "number_of_memory_tokens = 4096 # @param {type:\"slider\", min:128, max:4096, step:4}\n",
        "temperature = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "\n",
        "#@markdown Other settings\n",
        "render_MIDI_to_audio = True # @param {type:\"boolean\"}\n",
        "\n",
        "print('=' * 70)\n",
        "print('Orchestrator Standard Continuation Model Generator')\n",
        "print('=' * 70)\n",
        "\n",
        "inp = [melody_chords_f[-number_of_memory_tokens:]] * number_of_batches_to_generate\n",
        "\n",
        "inp = torch.LongTensor(inp).cuda()\n",
        "\n",
        "with ctx:\n",
        "  out = model.generate(inp,\n",
        "                        384,\n",
        "                        temperature=temperature,\n",
        "                        return_prime=False,\n",
        "                        verbose=True)\n",
        "\n",
        "out0 = out.tolist()\n",
        "\n",
        "print('=' * 70)\n",
        "print('Done!')\n",
        "#======================================================================\n",
        "print('=' * 70)\n",
        "print('Rendering results...')\n",
        "\n",
        "for i in range(number_of_batches_to_generate):\n",
        "\n",
        "  print('=' * 70)\n",
        "  print('Batch #', i)\n",
        "  print('=' * 70)\n",
        "\n",
        "  out1 = out0[i]\n",
        "\n",
        "  print('Sample INTs', out1[:12])\n",
        "  print('=' * 70)\n",
        "\n",
        "  if len(out) != 0:\n",
        "\n",
        "      song = out1\n",
        "      song_f = []\n",
        "      tim = 0\n",
        "      dur = 0\n",
        "      vel = 0\n",
        "      pitch = 0\n",
        "      channel = 0\n",
        "\n",
        "      son = []\n",
        "      song1 = []\n",
        "\n",
        "      for s in song:\n",
        "        if s > 256 and s < (12*9)+640:\n",
        "          son.append(s)\n",
        "        else:\n",
        "          if len(son) == 4:\n",
        "            song1.append(son)\n",
        "          son = []\n",
        "          son.append(s)\n",
        "\n",
        "\n",
        "      for ss in song1:\n",
        "\n",
        "          if ss[0] < 128:\n",
        "            tim += ss[0] * 8\n",
        "\n",
        "          dur = (ss[1]-256) * 16\n",
        "\n",
        "          if (ss[2]-384) > 128:\n",
        "            pitch = (ss[2]-384) - 128\n",
        "          else:\n",
        "            pitch = (ss[2]-384)\n",
        "\n",
        "\n",
        "          channel = (ss[3]-640) // 9\n",
        "          vel = ((ss[3]-640) % 9) * 15\n",
        "\n",
        "          song_f.append(['note', tim, dur, channel, pitch, vel ])\n",
        "\n",
        "      detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
        "                                                          output_signature = 'Orchestrator',\n",
        "                                                          output_file_name = '/content/Orchestrator-Music-Composition_'+str(i),\n",
        "                                                          track_name='Project Los Angeles',\n",
        "                                                          list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],                                                        number_of_ticks_per_quarter=500)\n",
        "\n",
        "      print('=' * 70)\n",
        "      print('Displaying resulting composition...')\n",
        "      print('=' * 70)\n",
        "\n",
        "      fname = '/content/Orchestrator-Music-Composition_'+str(i)\n",
        "\n",
        "      x = []\n",
        "      y =[]\n",
        "      c = []\n",
        "\n",
        "      colors = ['red', 'yellow', 'green', 'cyan', 'blue', 'pink', 'orange', 'purple', 'gray', 'white', 'gold', 'silver']\n",
        "\n",
        "      for s in song_f:\n",
        "        x.append(s[1] / 1000)\n",
        "        y.append(s[4])\n",
        "        c.append(colors[s[3]])\n",
        "\n",
        "      if render_MIDI_to_audio:\n",
        "        FluidSynth(\"/usr/share/sounds/sf2/FluidR3_GM.sf2\", 16000).midi_to_audio(str(fname + '.mid'), str(fname + '.wav'))\n",
        "        display(Audio(str(fname + '.wav'), rate=16000))\n",
        "\n",
        "      plt.figure(figsize=(14,5))\n",
        "      ax=plt.axes(title=fname)\n",
        "      ax.set_facecolor('black')\n",
        "\n",
        "      plt.scatter(x,y, c=c)\n",
        "      plt.xlabel(\"Time\")\n",
        "      plt.ylabel(\"Pitch\")\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose one generated block to add to the composition\n",
        "block_action = \"add_last_generated_block\" #@param [\"add_last_generated_block\", \"remove_last_added_block\"]\n",
        "add_block_with_batch_number = 0 #@param {type:\"slider\", min:0, max:15, step:1}\n",
        "render_MIDI_to_audio = False # @param {type:\"boolean\"}\n",
        "\n",
        "print('=' * 70)\n",
        "\n",
        "if block_action == 'add_last_generated_block':\n",
        "  melody_chords_f.extend(out0[add_block_with_batch_number])\n",
        "  print('Block added!')\n",
        "else:\n",
        "  melody_chords_f = melody_chords_f[:max(number_of_prime_tokens, (len(melody_chords_f)-384))]\n",
        "  print('Block removed!')\n",
        "\n",
        "print('=' * 70)\n",
        "print('Composition now has', (len(melody_chords_f) // 4), 'notes')\n",
        "print('Composition now has', len(melody_chords_f), 'tokens')\n",
        "\n",
        "\n",
        "print('=' * 70)\n",
        "print('Sample INTs', out1[:12])\n",
        "print('=' * 70)\n",
        "\n",
        "if len(melody_chords_f) != 0:\n",
        "\n",
        "    song = melody_chords_f\n",
        "    song_f = []\n",
        "    tim = 0\n",
        "    dur = 0\n",
        "    vel = 0\n",
        "    pitch = 0\n",
        "    channel = 0\n",
        "\n",
        "    son = []\n",
        "    song1 = []\n",
        "\n",
        "    for s in song:\n",
        "      if s > 256 and s < (12*9)+640:\n",
        "        son.append(s)\n",
        "      else:\n",
        "        if len(son) == 4:\n",
        "          song1.append(son)\n",
        "        son = []\n",
        "        son.append(s)\n",
        "\n",
        "\n",
        "    for ss in song1:\n",
        "\n",
        "        if ss[0] < 128:\n",
        "          tim += ss[0] * 8\n",
        "\n",
        "        dur = (ss[1]-256) * 16\n",
        "\n",
        "        if (ss[2]-384) > 128:\n",
        "          pitch = (ss[2]-384) - 128\n",
        "        else:\n",
        "          pitch = (ss[2]-384)\n",
        "\n",
        "\n",
        "        channel = (ss[3]-640) // 9\n",
        "        vel = ((ss[3]-640) % 9) * 15\n",
        "\n",
        "        song_f.append(['note', tim, dur, channel, pitch, vel ])\n",
        "\n",
        "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
        "                                                        output_signature = 'Orchestrator',\n",
        "                                                        output_file_name = '/content/Orchestrator-Music-Composition',\n",
        "                                                        track_name='Project Los Angeles',\n",
        "                                                        list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],                                                        number_of_ticks_per_quarter=500)\n",
        "\n",
        "    print('=' * 70)\n",
        "    print('Displaying resulting composition...')\n",
        "    print('=' * 70)\n",
        "\n",
        "    fname = '/content/Orchestrator-Music-Composition'\n",
        "\n",
        "    x = []\n",
        "    y =[]\n",
        "    c = []\n",
        "\n",
        "    colors = ['red', 'yellow', 'green', 'cyan', 'blue', 'pink', 'orange', 'purple', 'gray', 'white', 'gold', 'silver']\n",
        "\n",
        "    if block_action == 'add_last_generated_block':\n",
        "      block_lines.append((song_f[-1][1] / 1000))\n",
        "    else:\n",
        "      if len(block_lines) > 1:\n",
        "        block_lines.pop()\n",
        "\n",
        "\n",
        "    for s in song_f:\n",
        "      x.append(s[1] / 1000)\n",
        "      y.append(s[4])\n",
        "      c.append(colors[s[3]])\n",
        "\n",
        "    if render_MIDI_to_audio:\n",
        "      FluidSynth(\"/usr/share/sounds/sf2/FluidR3_GM.sf2\", 16000).midi_to_audio(str(fname + '.mid'), str(fname + '.wav'))\n",
        "      display(Audio(str(fname + '.wav'), rate=16000))\n",
        "\n",
        "    plt.figure(figsize=(14,5))\n",
        "    ax=plt.axes(title=fname)\n",
        "    ax.set_facecolor('black')\n",
        "\n",
        "    plt.scatter(x,y, c=c)\n",
        "\n",
        "    for bl in block_lines:\n",
        "      ax.axvline(x=bl, c='w')\n",
        "\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Pitch\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3YdlOR_9TdYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Congrats! You did it! :)"
      ],
      "metadata": {
        "id": "eoWDEy6CwDr6"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}