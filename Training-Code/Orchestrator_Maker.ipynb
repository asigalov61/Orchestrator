{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrator Maker (ver. 0.5)\n",
    "\n",
    "***\n",
    "\n",
    "Powered by tegridy-tools: https://github.com/asigalov61/tegridy-tools\n",
    "\n",
    "***\n",
    "\n",
    "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please excercise great humility, care, and respect. https://www.nscai.gov/\n",
    "\n",
    "***\n",
    "\n",
    "#### Project Los Angeles\n",
    "\n",
    "#### Tegridy Code 2022\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3rABEpKCO02"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viHgEaNACPTs"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/asigalov61/tegridy-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vK40g6V_BTNj"
   },
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "!pip install einops\n",
    "!pip install torch-summary\n",
    "#!pip install sklearn\n",
    "!pip install tqdm\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzCOZU_gBiQV"
   },
   "outputs": [],
   "source": [
    "# Load modules and make data dir\n",
    "\n",
    "print('Loading modules...')\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import secrets\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsummary import summary\n",
    "from sklearn import metrics\n",
    "\n",
    "%cd /notebooks/tegridy-tools/tegridy-tools/\n",
    "\n",
    "import TMIDIX\n",
    "\n",
    "%cd /notebooks/tegridy-tools/tegridy-tools/LWA-Transformer\n",
    "\n",
    "from lwa_transformer import *\n",
    "\n",
    "%cd /notebooks/\n",
    "\n",
    "if not os.path.exists('/notebooks/INTS'):\n",
    "    os.makedirs('/notebooks/INTS')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download training data pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestrator Multi-Instrumental Training Data Pack\n",
    "%cd /notebooks/INTS/\n",
    "!wget --no-check-certificate -O 'Orchestrator-Training-Data.zip' \"https://onedrive.live.com/download?cid=8A0D502FC99C608F&resid=8A0D502FC99C608F%2118782&authkey=AAPFXo9AlZTYNGE\"\n",
    "!unzip 'Orchestrator-Training-Data.zip'\n",
    "!rm 'Orchestrator-Training-Data.zip'\n",
    "%cd /notebooks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ujn-SVVChZV"
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "\n",
    "dataset_addr = \"/notebooks/INTS\"\n",
    "\n",
    "filez = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(dataset_addr):\n",
    "    filez += [os.path.join(dirpath, file) for file in filenames]\n",
    "print('=' * 70)\n",
    "\n",
    "filez.sort()\n",
    "\n",
    "print('Loading training data... Please wait...')\n",
    "\n",
    "train_data = torch.Tensor()\n",
    "\n",
    "for f in tqdm.tqdm(filez):\n",
    "    train_data = torch.cat((train_data, torch.Tensor(pickle.load(open(f, 'rb')))))\n",
    "    print('Loaded file:', f)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtHSA8MeCpGU"
   },
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLg-NI2BCrtQ"
   },
   "outputs": [],
   "source": [
    "train_data[:15], train_data[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MPxk8PFCtJ1"
   },
   "outputs": [],
   "source": [
    "# Setup model\n",
    "\n",
    "# constants\n",
    "\n",
    "SEQ_LEN = 4096\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "NUM_BATCHES = (len(train_data) // SEQ_LEN // BATCH_SIZE) * NUM_EPOCHS\n",
    "\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "VALIDATE_EVERY  = 100\n",
    "SAVE_EVERY = 500\n",
    "GENERATE_EVERY  = 200\n",
    "PRINT_STATS_EVERY = 50\n",
    "\n",
    "GENERATE_LENGTH = 32\n",
    "\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "# instantiate the model\n",
    "\n",
    "model = LocalTransformer(\n",
    "    num_tokens = 774,\n",
    "    dim = 1024,\n",
    "    depth = 24,\n",
    "    causal = True,\n",
    "    local_attn_window_size = 512,\n",
    "    max_seq_len = SEQ_LEN\n",
    ").cuda()\n",
    "\n",
    "print('Done!')\n",
    "      \n",
    "summary(model)\n",
    "\n",
    "# Dataloader\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # random sampling\n",
    "        idx = secrets.randbelow(self.data.size(0) - self.seq_len - 1)      \n",
    "        full_seq = self.data[idx: idx + self.seq_len + 1].long()\n",
    "        \n",
    "        return full_seq.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "train_dataset = MusicDataset(train_data, SEQ_LEN)\n",
    "val_dataset   = MusicDataset(train_data, SEQ_LEN)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgTifRdrCxUU"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='Training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        loss, acc = model(next(train_loader), return_loss = True)\n",
    "        loss.backward()\n",
    "        \n",
    "    if i % PRINT_STATS_EVERY == 0:\n",
    "        print(f'Training loss: {loss.item()}')\n",
    "        print(f'Training acc: {acc.item()}')\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    train_accs.append(acc.item())\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc = model(next(val_loader), return_loss = True)\n",
    "            \n",
    "            print(f'Validation loss: {val_loss.item()}')\n",
    "            print(f'Validation acc: {val_acc.item()}')\n",
    "            \n",
    "            val_losses.append(val_loss.item())\n",
    "            val_accs.append(val_acc.item())\n",
    "            \n",
    "            print('Plotting training loss graph...')\n",
    "            \n",
    "            tr_loss_list = train_losses\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "            \n",
    "            print('Plotting training acc graph...')\n",
    "            \n",
    "            tr_loss_list = train_accs\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "            \n",
    "            print('Plotting validation loss graph...')\n",
    "            tr_loss_list = val_losses\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "            \n",
    "            print('Plotting validation acc graph...')\n",
    "            tr_loss_list = val_accs\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "\n",
    "    if i % GENERATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        \n",
    "        print(inp)\n",
    "\n",
    "        sample = model.generate(inp[None, ...], GENERATE_LENGTH)\n",
    "        \n",
    "        print(sample)\n",
    "        \n",
    "    if i % SAVE_EVERY == 0:\n",
    "        \n",
    "        print('Saving model progress. Please wait...')\n",
    "        print('model_checkpoint_' + str(i) + '_steps_' + str(round(float(train_losses[-1]), 4)) + '_loss.pth')\n",
    "        \n",
    "        fname = '/notebooks/model_checkpoint_'  + str(i) + '_steps_' + str(round(float(train_losses[-1]), 4)) + '_loss.pth'\n",
    "        \n",
    "        torch.save(model.state_dict(), fname)\n",
    "        \n",
    "        data = [train_losses, train_accs, val_losses, val_accs]\n",
    "\n",
    "        TMIDIX.Tegridy_Any_Pickle_File_Writer(data, '/notebooks/losses_accs')\n",
    "        \n",
    "        print('Done!')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1ba3J9fIYOV"
   },
   "outputs": [],
   "source": [
    "print('Saving model progress. Please wait...')\n",
    "print('model_checkpoint_' + str(i) + '_steps_' + str(round(float(train_losses[-1]), 4)) + '_loss.pth')\n",
    "\n",
    "fname = '/notebooks/model_checkpoint_'  + str(i) + '_steps_' + str(round(float(train_losses[-1]), 4)) + '_loss.pth'\n",
    "\n",
    "torch.save(model.state_dict(), fname)\n",
    "\n",
    "print('Done!')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vE5Z15fCz1M"
   },
   "outputs": [],
   "source": [
    "# Save training loss graph\n",
    "\n",
    "plt.plot([i for i in range(len(train_losses))] ,train_losses, 'b')\n",
    "plt.savefig('/notebooks/training_loss_graph.png')\n",
    "plt.close()\n",
    "print('Done!')\n",
    "\n",
    "# Save training acc graph\n",
    "\n",
    "plt.plot([i for i in range(len(train_accs))] ,train_accs, 'b')\n",
    "plt.savefig('/notebooks/training_acc_graph.png')\n",
    "plt.close()\n",
    "print('Done!')\n",
    "\n",
    "# Save validation loss graph\n",
    "\n",
    "plt.plot([i for i in range(len(val_losses))] ,val_losses, 'b')\n",
    "plt.savefig('/notebooks/validation_loss_graph.png')\n",
    "plt.close()\n",
    "print('Done!')\n",
    "\n",
    "# Save validation acc graph\n",
    "\n",
    "plt.plot([i for i in range(len(val_accs))] ,val_accs, 'b')\n",
    "plt.savefig('/notebooks/validation_acc_graph.png')\n",
    "plt.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [train_losses, train_accs, val_losses, val_accs]\n",
    "\n",
    "TMIDIX.Tegridy_Any_Pickle_File_Writer(data, '/notebooks/losses_accs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congrats! You did it! :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
